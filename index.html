<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NLP with Deep Learning - UNAM</title>
    <link rel="stylesheet" href="css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@400;600;700&family=Source+Sans+Pro:wght@400;600&display=swap" rel="stylesheet">
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <img src="assets/unam-logo-png-transparent.png" alt="UNAM Logo" class="university-logo">
                    <div class="logo-text">
                        <h1>Natural Language Processing with Deep Learning</h1>
                        <p class="subtitle">School of Engineering - UNAM ¬∑ Semester 2026-2</p>
                    </div>
                </div>
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
                    <span class="sun-icon">‚òÄÔ∏è</span>
                    <span class="moon-icon">üåô</span>
                </button>
            </div>
        </div>
    </header>

    <nav class="nav">
        <div class="container">
            <ul class="nav-links">
                <li><a href="#overview">Overview</a></li>
                <li><a href="#syllabus">Syllabus</a></li>
                <li><a href="prerequisites.html">Prerequisites</a></li>
                <li><a href="guidelines.html">Guidelines</a></li>
                <li><a href="#instructor">Instructor</a></li>
                <li><a href="testimonials.html">Student Experiences</a></li>
            </ul>
        </div>
    </nav>

    <main class="main">
        <section id="overview" class="section">
            <div class="container">
                <h2>Course Overview</h2>
                <div class="overview-grid">
                    <div class="info-card">
                        <h3>Course ID</h3>
                        <p>2950</p>
                    </div>
                    <div class="info-card">
                        <h3>Credits</h3>
                        <p>6 Credits</p>
                    </div>
                    <div class="info-card">
                        <h3>Program</h3>
                        <p>Computer Engineering</p>
                    </div>
                    <div class="info-card repo-card">
                        <h3>Repository</h3>
                        <p><a href="https://github.com/milmor/NLP" target="_blank">github.com/milmor/NLP</a></p>
                    </div>
                </div>
                <div class="description">
                    <p>This course provides a comprehensive introduction to Natural Language Processing using Deep Learning techniques. Starting with machine learning fundamentals for text classification, students progress through neural network architectures (perceptrons, RNNs, LSTMs, Transformers) to state-of-the-art models like BERT and GPT. The course also covers language and vision models, including CNNs, VAEs, GANs, and diffusion models for text-to-image generation.</p>
                </div>

                <div class="prerequisites-warning">
                    <h3>‚ö†Ô∏è Prerequisites Required</h3>
                    <p>This course requires strong foundations in:</p>
                    <ul class="prereq-list">
                        <li><strong>Linear Algebra</strong> ‚Äì matrices, vectors, transformations</li>
                        <li><strong>Calculus</strong> ‚Äì derivatives, gradients, chain rule</li>
                        <li><strong>Probability & Statistics</strong> ‚Äì distributions, Bayes' theorem</li>
                        <li><strong>Programming</strong> ‚Äì Python proficiency required</li>
                    </ul>
                    <p class="prereq-note">See <a href="prerequisites.html">student background data</a> from previous semesters.</p>
                </div>

                <div class="course-objectives">
                    <h3>Course Objectives</h3>
                    <ul>
                        <li>Understand the fundamental concepts of deep learning and their application in NLP, such as text representation and neural networks.</li>
                        <li>Acquire practical skills for developing and training NLP models using deep learning techniques.</li>
                        <li>Apply the knowledge gained to real-world NLP use cases, such as sentiment analysis, machine translation, text generation, and data generation from natural language.</li>
                    </ul>
                </div>

            </div>
        </section>

        <section id="syllabus" class="section section-alt">
            <div class="container">
                <h2>Course Syllabus</h2>
                <p class="section-intro syllabus-note">The program and dates below are subject to change.</p>
                <div class="syllabus-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Week</th>
                                <th>Topic</th>
                                <th>Materials</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1</td>
                                <td>
                                    <strong>Machine Learning & NLP Fundamentals</strong>
                                    <p>Text classification, bag of words, n-grams, Naive Bayes, maximum likelihood, logistic regression</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a> (Hastie et al.), ¬ß4.4 Logistic Regression, p. 119
                                </td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>
                                    <strong>Text Representation & Evaluation</strong>
                                    <p>TF-IDF, PCA, stemming, lemmatization, Zipf's law, precision/recall/F1</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a> (Hastie et al.), ¬ß14.5 Principal Components, Curves and Surfaces, p. 534
                                </td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>
                                    <strong>Introduction to Neural Networks</strong>
                                    <p>Perceptron, multilayer perceptron, backpropagation</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a> (Hastie et al.), ¬ß4.5 Separating Hyperplanes, p. 129<br>
                                    2. <a href="https://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">Derivatives, Backpropagation, and Vectorization</a><br>
                                    3. <a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf" target="_blank" rel="noopener">Computing Neural Network Gradients</a>
                                </td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>
                                    <strong>Neural Network Training</strong>
                                    <p>Activation functions, entropy, optimizers (SGD, Adam), regularization</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/1412.6980">Adam: A Method for Stochastic Optimization</a><br>
                                    2. <a href="https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>
                                </td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>
                                    <strong>Word Embeddings</strong>
                                    <p>Word vectors, Word2Vec, distributed representations</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/1301.3781">Efficient Estimation of Word Representations in Vector Space</a> (word2vec paper)<br>
                                    <span class="deadline-note"><strong>Deadline:</strong> <a href="guidelines.html">Research project proposal</a> due</span>
                                </td>
                            </tr>
                            <tr>
                                <td>6</td>
                                <td>
                                    <strong>Recurrent Neural Networks</strong>
                                    <p>RNNs, backpropagation through time, LSTM, GRU</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Nets</a><br>
                                    2. <a href="https://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training Recurrent Neural Networks</a>
                                </td>
                            </tr>
                            <tr>
                                <td>7</td>
                                <td>
                                    <strong>Language Models & Text Generation</strong>
                                    <p>Language modeling, sequence generation, sampling strategies</p>
                                </td>
                                <td></td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>
                                    <strong>Sequence-to-Sequence & Attention</strong>
                                    <p>Seq2seq for translation, attention mechanism</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/1409.3215">Sequence to Sequence Learning with Neural Networks</a> (seq2seq paper)<br>
                                    2. <a href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> (Bahdanau attention paper)
                                </td>
                            </tr>
                            <tr>
                                <td>9</td>
                                <td>
                                    <strong>Transformers</strong>
                                    <p>Self-attention, multi-head attention, positional encoding</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a> (Transformer paper)<br>
                                    2. <a href="https://arxiv.org/pdf/1607.06450">Layer Normalization</a>                                </td>
                            </tr>
                            <tr>
                                <td>10</td>
                                <td>
                                    <strong>Pre-trained Models: BERT & GPT</strong>
                                    <p>Transfer learning, fine-tuning, masked language modeling</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
                                    2. <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a> (GPT paper)
                                </td>
                            </tr>
                            <tr>
                                <td>11</td>
                                <td>
                                    <strong>Advanced NLP Techniques</strong>
                                    <p>LoRA, FAISS, Retrieval-Augmented Generation (RAG)</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a><br>
                                    2. <a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>
                                </td>
                            </tr>
                            <tr>
                                <td>12</td>
                                <td>
                                    <strong>Language & Vision: CNNs and Captioning</strong>
                                    <p>Convolutional neural networks, image description generation</p>
                                </td>
                                <td></td>
                            </tr>
                            <tr>
                                <td>13</td>
                                <td>
                                    <strong>Generative Models: VAEs & GANs</strong>
                                    <p>Variational autoencoders, generative adversarial networks</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/1312.6114">Auto-Encoding Variational Bayes</a><br>
                                    2. <a href="https://arxiv.org/pdf/1406.2661">Generative Adversarial Networks</a>
                                </td>
                            </tr>
                            <tr>
                                <td>14</td>
                                <td>
                                    <strong>Generative Models: Diffusion</strong>
                                    <p>Diffusion models, DALL-E, Stable Diffusion, text-to-image generation</p>
                                </td>
                                <td>
                                    <strong>Readings:</strong><br>
                                    1. <a href="https://arxiv.org/pdf/2006.11239">Denoising Diffusion Probabilistic Models</a> (DDPM paper)<br>
                                    2. <a href="https://arxiv.org/pdf/2105.05233">Diffusion Models Beat GANs on Image Synthesis</a><br>
                                    3. <a href="https://arxiv.org/pdf/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a> (Stable Diffusion paper)<br>
                                    4. <a href="https://arxiv.org/pdf/2212.09748">Scalable Diffusion Models with Transformers</a> (DiT paper)<br>
                                    5. <a href="https://arxiv.org/pdf/2207.12598">Classifier-Free Diffusion Guidance</a>
                                </td>
                            </tr>
                            <tr>
                                <td>15</td>
                                <td>
                                    <strong>Project Presentations</strong>
                                    <p>Student project demos and discussions</p>
                                </td>
                                <td><a href="guidelines.html">Guidelines</a></td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div id="programming-assignments" class="programming-assignments">
                    <h3>Programming Assignments</h3>
                    <p class="section-intro syllabus-note">The list below is subject to change.</p>
                    <ol class="program-list">
                        <li>Program 1: Machine Learning & NLP Fundamentals</li>
                        <li>Program 2: Word Embeddings, RNNs, Sequence-to-Sequence and Transformers</li>
                        <li>Program 3: Advanced Topics (e.g. advanced Transformers)</li>
                    </ol>
                    <ul class="program-rules">
                        <li>Each programming assignment is worth <strong>15%</strong> of the final grade.</li>
                        <li>Students are encouraged to work in teams of 2.</li>
                        <li>There will be two to four programs in total during the semester. Late submissions are not accepted.</li>
                    </ul>
                </div>

                <div class="course-details-grid">
                    <div class="detail-card">
                        <h4>Grading</h4>
                        <ul class="grading-list">
                            <li><span><a href="guidelines.html">Research Project</a></span><span>40%</span></li>
                            <li><span><a href="#programming-assignments">Programming Assignments</a></span><span>45%</span></li>
                            <li><span>Class Exercises</span><span>15%</span></li>
                        </ul>
                    </div>
                    <div class="detail-card">
                        <h4>Grading Scale</h4>
                        <ul class="scale-list">
                            <li><span>‚â• 95</span><span>10</span></li>
                            <li><span>85 - 94</span><span>9</span></li>
                            <li><span>75 - 84</span><span>8</span></li>
                            <li><span>65 - 74</span><span>7</span></li>
                            <li><span>60 - 64</span><span>6</span></li>
                            <li><span>25 - 59</span><span>5</span></li>
                            <li><span>‚â§ 25</span><span>NP</span></li>
                        </ul>
                    </div>
                    <div class="detail-card bibliography">
                        <h4>Bibliography</h4>
                        <p><strong>Machine Learning:</strong></p>
                        <ul>
                            <li>Hastie et al. <em><a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a></em>, Springer, 2009</li>
                        </ul>
                        <p><strong>Neural Networks:</strong></p>
                        <ul>
                            <li>Goodfellow et al. <em><a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a></em>, MIT Press, 2016</li>
                            <li>Zhang et al. <em><a href="https://d2l.ai/" target="_blank">Dive into Deep Learning</a></em>, 2021</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section id="testimonials" class="section">
            <div class="container">
                <h2>Student Experiences</h2>
                <p class="section-intro">Hear what previous students have to say about their experience in this course.</p>
                <div class="testimonials-grid">
                    <div class="testimonial-card">
                        <div class="quote">"La materia fue mucho m√°s interesante y entretenida de lo que llegu√© a pensar."</div>
                        <div class="author">
                            <div class="author-info">
                                <strong>Anonymous Student</strong>
                                <span>Semester 2026-1</span>
                            </div>
                        </div>
                    </div>
                    <div class="testimonial-card">
                        <div class="quote">"Es muy buen profesor y es muy bueno en la materia, sabe mucho y muchas gracias por todo."</div>
                        <div class="author">
                            <div class="author-info">
                                <strong>Anonymous Student</strong>
                                <span>Semester 2025-2</span>
                            </div>
                        </div>
                    </div>
                    <div class="testimonial-card">
                        <div class="quote">"Excelente curso, complicado pero extremadamente √∫til e interesante."</div>
                        <div class="author">
                            <div class="author-info">
                                <strong>Anonymous Student</strong>
                                <span>Semester 2024-1</span>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="see-all-link">
                    <a href="testimonials.html" class="btn-primary">See all experiences ‚Üí</a>
                </div>
            </div>
        </section>

        <section id="instructor" class="section section-alt">
            <div class="container">
                <h2>Instructor</h2>
                <div class="instructor-card">
                    <div class="instructor-info">
                        <h3>Prof. Emilio Morales</h3>
                        <p class="title">The School of Engineering of the National Autonomous University of Mexico</p>
                        <p class="bio">Research interests include natural language processing, deep learning, diffusion models, GANs, music generation, audio and video processing.</p>
                        <div class="contact-info">
                            <p><strong>Email:</strong> <a href="mailto:emilio.morales@ciencias.unam.mx">emilio.morales@ciencias.unam.mx</a></p>
                            <p><strong>Institution:</strong> School of Engineering, UNAM</p>
                            <p><strong>Location:</strong> Ciudad Universitaria, CDMX</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 NLP with Deep Learning - UNAM. All rights reserved.</p>
            <p>Hosted on <a href="https://pages.github.com/" target="_blank">GitHub Pages</a></p>
        </div>
    </footer>

    <script src="js/script.js"></script>
</body>
</html>
